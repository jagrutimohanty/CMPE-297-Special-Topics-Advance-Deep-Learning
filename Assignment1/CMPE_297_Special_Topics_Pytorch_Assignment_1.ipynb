{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMPE-297-Special Topics-Pytorch-Assignment-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jagrutimohanty/CMPE-297-Special-Topics-Advance-Deep-Learning/blob/main/Assignment1/CMPE_297_Special_Topics_Pytorch_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LAgOWnKMbZX"
      },
      "source": [
        "# Name : Jagruti Mohanty\n",
        "Assignment - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9ZEjxiNQTKv"
      },
      "source": [
        "## Broilerplate setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwoPOoaBb0oA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87343c7f-81fc-46ad-970f-5c61fc13a947"
      },
      "source": [
        "# Getting the dataset from the github repo of imagenet 5 categories\n",
        "!git clone https://github.com/thunderInfy/imagenet-5-categories.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'imagenet-5-categories'...\n",
            "remote: Enumerating objects: 1532, done.\u001b[K\n",
            "remote: Total 1532 (delta 0), reused 0 (delta 0), pack-reused 1532\u001b[K\n",
            "Receiving objects: 100% (1532/1532), 88.56 MiB | 1.04 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HfY5issLHJg"
      },
      "source": [
        "# Some useful imports to get resnet SimCLR results folder\n",
        "import requests\n",
        "import zipfile\n",
        "import io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyonhZFdVwhn"
      },
      "source": [
        "# Getting simCLR results folder\n",
        "r = requests.get('https://github.com/thunderInfy/resnet-simclr/blob/master/results.zip?raw=true')\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eKrpS6OBZHW"
      },
      "source": [
        "# Useful imports\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms as T\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "tsne = TSNE()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKUuE5E_DXNU"
      },
      "source": [
        "# device is set to cuda if cuda is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "map_location=torch.device('cpu')\n",
        "\n",
        "# root folder is the name of the folder where data is contained\n",
        "root_folder = 'imagenet-5-categories'\n",
        "\n",
        "train_names = sorted(os.listdir(root_folder + '/train'))\n",
        "test_names = sorted(os.listdir(root_folder + '/test'))\n",
        "\n",
        "# setting random seed to ensure the same 10% labelled data is used when training the linear classifier\n",
        "random.seed(0)\n",
        "\n",
        "names_train_10_percent = random.sample(train_names, len(train_names) // 10)\n",
        "names_train = random.sample(train_names, len(train_names))\n",
        "names_test = random.sample(test_names, len(test_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojqkmjNmMQts"
      },
      "source": [
        "## Data loading for training and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYwcOUWbBk5l"
      },
      "source": [
        "# defining a mapping between class names and numbers\n",
        "mapping = {'car': 0, 'dog': 1, 'elephant': 2, 'cat': 3, 'airplane': 4}\n",
        "inverse_mapping = ['car', 'dog', 'elephant', 'cat', 'airplane']\n",
        "\n",
        "# getting labels based on filenames, note that the filenames themselves contain classnames\n",
        "# also note that these labels won't be used to actually train the base model\n",
        "# these are just for visualization purposes\n",
        "labels_train = [mapping[x.split('_')[0]] for x in names_train]\n",
        "labels_test = [mapping[x.split('_')[0]] for x in names_test]\n",
        "\n",
        "# these 10 percent labels will be used for training the linear classifer\n",
        "labels_train_10_percent = [mapping[x.split('_')[0]] for x in names_train_10_percent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbxUjK9ZD90A"
      },
      "source": [
        "# A function to perform color distortion in images\n",
        "# It is used in SimCLR alongwith random resized cropping\n",
        "# Here, s is the strength of color distortion.\n",
        "\n",
        "def get_color_distortion(s=1.0):\n",
        "    color_jitter = T.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
        "    rnd_color_jitter = T.RandomApply([color_jitter], p=0.8)\n",
        "    \n",
        "    # p is the probability of grayscale, here 0.2\n",
        "    rnd_gray = T.RandomGrayscale(p=0.2)\n",
        "    color_distort = T.Compose([rnd_color_jitter, rnd_gray])\n",
        "    \n",
        "    return color_distort"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEP9MZKoE463"
      },
      "source": [
        "# this is the dataset class\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, root_dir, filenames, labels, mutation=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.file_names = filenames\n",
        "        self.labels = labels\n",
        "        self.mutation = mutation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def tensorify(self, img):\n",
        "        res = T.ToTensor()(img)\n",
        "        res = T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(res)\n",
        "        return res\n",
        "\n",
        "    def mutate_image(self, img):\n",
        "        res = T.RandomResizedCrop(224)(img)\n",
        "        res = get_color_distortion(1)(res)\n",
        "        return res\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir, self.file_names[idx])\n",
        "        image = Image.open(img_name)\n",
        "        label = self.labels[idx]\n",
        "        image = T.Resize((250, 250))(image)\n",
        "\n",
        "        if self.mutation:\n",
        "            image1 = self.mutate_image(image)\n",
        "            image1 = self.tensorify(image1)\n",
        "            image2 = self.mutate_image(image)\n",
        "            image2 = self.tensorify(image2)\n",
        "            sample = {'image1': image1, 'image2': image2, 'label': label}\n",
        "        else:\n",
        "            image = T.Resize((224, 224))(image)\n",
        "            image = self.tensorify(image)\n",
        "            sample = {'image': image, 'label': label}\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IqfsF8MHYnb"
      },
      "source": [
        "# datasets\n",
        "training_dataset_mutated = MyDataset(root_folder + '/train', names_train, labels_train, mutation=True)\n",
        "training_dataset = MyDataset(root_folder + '/train', names_train_10_percent, labels_train_10_percent, mutation=False)\n",
        "testing_dataset = MyDataset(root_folder + '/test', names_test, labels_test, mutation=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ8eFhqbHeTh"
      },
      "source": [
        "# dataloaders\n",
        "dataloader_training_dataset_mutated = DataLoader(training_dataset_mutated, batch_size=250, shuffle=True, num_workers=2)\n",
        "dataloader_training_dataset = DataLoader(training_dataset, batch_size=125, shuffle=True, num_workers=2)\n",
        "dataloader_testing_dataset = DataLoader(testing_dataset, batch_size=250, shuffle=True, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOyJNjeQQiA-"
      },
      "source": [
        "## Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJUXtxYiHnxz"
      },
      "source": [
        "# defining our deep learning architecture\n",
        "resnet = resnet18(pretrained=False)\n",
        "\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "    ('fc1', nn.Linear(resnet.fc.in_features, 100)),\n",
        "    ('added_relu1', nn.ReLU(inplace=True)),\n",
        "    ('fc2', nn.Linear(100, 50)),\n",
        "    ('added_relu2', nn.ReLU(inplace=True)),\n",
        "    ('fc3', nn.Linear(50, 25))\n",
        "]))\n",
        "\n",
        "resnet.fc = classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HQJywp9Hq_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd9c2c61-0d76-4639-811f-313c318e5597"
      },
      "source": [
        "# can view the summary if you like\n",
        "summary(resnet, (3, 224, 224))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                  [-1, 100]          51,300\n",
            "             ReLU-69                  [-1, 100]               0\n",
            "           Linear-70                   [-1, 50]           5,050\n",
            "             ReLU-71                   [-1, 50]               0\n",
            "           Linear-72                   [-1, 25]           1,275\n",
            "================================================================\n",
            "Total params: 11,234,137\n",
            "Trainable params: 11,234,137\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 42.85\n",
            "Estimated Total Size (MB): 106.22\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bP5-FOJHyaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52601c9-27a6-4e73-d6b8-f8a1eb670a15"
      },
      "source": [
        "# moving the resnet architecture to device\n",
        "resnet.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (fc1): Linear(in_features=512, out_features=100, bias=True)\n",
              "    (added_relu1): ReLU(inplace=True)\n",
              "    (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
              "    (added_relu2): ReLU(inplace=True)\n",
              "    (fc3): Linear(in_features=50, out_features=25, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtBEidRXIAyP"
      },
      "source": [
        "# Code for NT-Xent Loss function, explained in more detail in the article\n",
        "\n",
        "tau = 0.05\n",
        "\n",
        "def loss_function(a, b):\n",
        "    a_norm = torch.norm(a, dim=1).reshape(-1, 1)\n",
        "    a_cap = torch.div(a, a_norm)\n",
        "    b_norm = torch.norm(b, dim=1).reshape(-1, 1)\n",
        "    b_cap = torch.div(b, b_norm)\n",
        "    a_cap_b_cap = torch.cat([a_cap, b_cap], dim=0)\n",
        "    a_cap_b_cap_transpose = torch.t(a_cap_b_cap)\n",
        "    b_cap_a_cap = torch.cat([b_cap, a_cap], dim=0)\n",
        "    sim = torch.mm(a_cap_b_cap, a_cap_b_cap_transpose)\n",
        "    sim_by_tau = torch.div(sim, tau)\n",
        "    exp_sim_by_tau = torch.exp(sim_by_tau)\n",
        "    sum_of_rows = torch.sum(exp_sim_by_tau, dim=1)\n",
        "    exp_sim_by_tau_diag = torch.diag(exp_sim_by_tau)\n",
        "    numerators = torch.exp(torch.div(torch.nn.CosineSimilarity()(a_cap_b_cap, b_cap_a_cap), tau))\n",
        "    denominators = sum_of_rows - exp_sim_by_tau_diag\n",
        "    num_by_den = torch.div(numerators, denominators)\n",
        "    neglog_num_by_den = -torch.log(num_by_den)\n",
        "    return torch.mean(neglog_num_by_den)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5OwRijMIOtc"
      },
      "source": [
        "# Defining data structures for storing training info\n",
        "\n",
        "losses_train = []\n",
        "num_epochs = 10\n",
        "\n",
        "# using SGD optimizer\n",
        "optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "if not os.path.exists('results'):\n",
        "    os.makedirs('results')\n",
        "\n",
        "# load pretrained model, optimizer and training losses file if model.pth file is available\n",
        "if(os.path.isfile(\"results/model.pth\")):\n",
        "    #resnet.load_state_dict(torch.load(\"results/model.pth\"))\n",
        "    resnet.load_state_dict(torch.load('results/model.pth',map_location ='cpu'))\n",
        "  #  optimizer.load_state_dict(torch.load(\"results/optimizer.pth\"))\n",
        "    optimizer.load_state_dict(torch.load('results/optimizer.pth',map_location ='cpu'))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['weight_decay'] = 1e-6\n",
        "        param_group['lr'] = 0.000003\n",
        "\n",
        "    temp = np.load(\"results/lossesfile.npz\")\n",
        "    losses_train = list(temp['arr_0'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZqREfH-Qs-K"
      },
      "source": [
        "## Training of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2UQmK5hIyPS"
      },
      "source": [
        "# Boolean variable on whether to perform training or not \n",
        "# Note that this training is unsupervised, it uses the NT-Xent Loss function\n",
        "\n",
        "TRAINING = False\n",
        "\n",
        "def get_mean_of_list(L):\n",
        "    return sum(L) / len(L)\n",
        "\n",
        "if TRAINING:\n",
        "    # get resnet in train mode\n",
        "    resnet.train()\n",
        "\n",
        "    # run a for loop for num_epochs\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # a list to store losses for each epoch\n",
        "        epoch_losses_train = []\n",
        "\n",
        "        # run a for loop for each batch\n",
        "        for (_, sample_batched) in enumerate(dataloader_training_dataset_mutated):\n",
        "            \n",
        "            # zero out grads\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # retrieve x1 and x2 the two image batches\n",
        "            x1 = sample_batched['image1']\n",
        "            x2 = sample_batched['image2']\n",
        "\n",
        "            # move them to the device\n",
        "            x1 = x1.to(device)\n",
        "            x2 = x2.to(device)\n",
        "\n",
        "            # get their outputs\n",
        "            y1 = resnet(x1)\n",
        "            y2 = resnet(x2)\n",
        "\n",
        "            # get loss value\n",
        "            loss = loss_function(y1, y2)\n",
        "            \n",
        "            # put that loss value in the epoch losses list\n",
        "            epoch_losses_train.append(loss.cpu().data.item())\n",
        "\n",
        "            # perform backprop on loss value to get gradient values\n",
        "            loss.backward()\n",
        "\n",
        "            # run the optimizer\n",
        "            optimizer.step()\n",
        "\n",
        "        # append mean of epoch losses to losses_train, essentially this will reflect mean batch loss\n",
        "        losses_train.append(get_mean_of_list(epoch_losses_train))\n",
        "\n",
        "        # Plot the training losses Graph and save it\n",
        "        fig = plt.figure(figsize=(10, 10))\n",
        "        sns.set_style('darkgrid')\n",
        "        plt.plot(losses_train)\n",
        "        plt.legend(['Training Losses'])\n",
        "        plt.savefig('losses.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Store model and optimizer files\n",
        "        torch.save(resnet.state_dict(), 'results/model.pth')\n",
        "        torch.save(optimizer.state_dict(), 'results/optimizer.pth')\n",
        "        np.savez(\"results/lossesfile\", np.array(losses_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TXIq55-Q7Kt"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7DE4cHeKah6"
      },
      "source": [
        "# a function used to plot t-SNE visualizations\n",
        "def plot_vecs_n_labels(v,labels,fname):\n",
        "    fig = plt.figure(figsize = (10, 10))\n",
        "    plt.axis('off')\n",
        "    sns.set_style(\"darkgrid\")\n",
        "    sns.scatterplot(v[:,0], v[:,1], hue=labels, legend='full', palette=sns.color_palette(\"bright\", 5))\n",
        "    plt.legend(['car', 'dog', 'elephant','cat','airplane'])\n",
        "    plt.savefig(fname)\n",
        "    plt.close()\n",
        "\n",
        "# Boolean variable to control whether to perform t-SNE visualization or not\n",
        "TSNEVIS = False\n",
        "\n",
        "if TSNEVIS:\n",
        "    # run resnet in eval mode\n",
        "    resnet.eval()\n",
        "\n",
        "    # get TSNE visualizations of 10% training dataset\n",
        "    for (_, sample_batched) in enumerate(dataloader_training_dataset):\n",
        "        x = sample_batched['image']\n",
        "        x = x.to(device)\n",
        "        y = resnet(x)\n",
        "        y_tsne = tsne.fit_transform(y.cpu().data)\n",
        "        labels = sample_batched['label']\n",
        "        plot_vecs_n_labels(y_tsne,labels,'tsne_train_last_layer.png')\n",
        "        x = None\n",
        "        y = None\n",
        "        y_tsne = None\n",
        "        sample_batched = None\n",
        "\n",
        "    # get TSNE visualizations of testing dataset\n",
        "    for (_, sample_batched) in enumerate(dataloader_testing_dataset):\n",
        "        x = sample_batched['image']\n",
        "        x = x.to(device)\n",
        "        y = resnet(x)\n",
        "        y_tsne = tsne.fit_transform(y.cpu().data)\n",
        "        labels = sample_batched['label']\n",
        "        plot_vecs_n_labels(y_tsne,labels,'tsne_test_last_layer.png')\n",
        "        x = None\n",
        "        y = None\n",
        "        y_tsne = None\n",
        "        sample_batched = None\n",
        "\n",
        "# Removing the last layer and the relu layer, we remove layers incrementally and look t-SNE visualizations\n",
        "resnet.fc = nn.Sequential(*list(resnet.fc.children())[:-2])\n",
        "\n",
        "if TSNEVIS:\n",
        "    for (_, sample_batched) in enumerate(dataloader_training_dataset):\n",
        "        x = sample_batched['image']\n",
        "        x = x.to(device)\n",
        "        y = resnet(x)\n",
        "        y_tsne = tsne.fit_transform(y.cpu().data)\n",
        "        labels = sample_batched['label']\n",
        "        plot_vecs_n_labels(y_tsne,labels,'tsne_train_second_last_layer.png')\n",
        "        x = None\n",
        "        y = None\n",
        "        y_tsne = None\n",
        "        sample_batched = None\n",
        "\n",
        "    for (_, sample_batched) in enumerate(dataloader_testing_dataset):\n",
        "        x = sample_batched['image']\n",
        "        x = x.to(device)\n",
        "        y = resnet(x)\n",
        "        y_tsne = tsne.fit_transform(y.cpu().data)\n",
        "        labels = sample_batched['label']\n",
        "        plot_vecs_n_labels(y_tsne,labels,'tsne_test_second_last_layer.png')\n",
        "        x = None\n",
        "        y = None\n",
        "        y_tsne = None\n",
        "        sample_batched = None\n",
        "\n",
        "# removing one more layer, our entire projection head will be removed after this\n",
        "resnet.fc = nn.Sequential(*list(resnet.fc.children())[:-1])\n",
        "\n",
        "if TSNEVIS:\n",
        "    for (_, sample_batched) in enumerate(dataloader_training_dataset):\n",
        "        x = sample_batched['image']\n",
        "        x = x.to(device)\n",
        "        y = resnet(x)\n",
        "        y_tsne = tsne.fit_transform(y.cpu().data)\n",
        "        labels = sample_batched['label']\n",
        "        plot_vecs_n_labels(y_tsne,labels,'tsne_hidden_train.png')\n",
        "        x = None\n",
        "        y = None\n",
        "        y_tsne = None\n",
        "        sample_batched = None\n",
        "\n",
        "    for (_, sample_batched) in enumerate(dataloader_testing_dataset):\n",
        "        x = sample_batched['image']\n",
        "        x = x.to(device)\n",
        "        y = resnet(x)\n",
        "        y_tsne = tsne.fit_transform(y.cpu().data)\n",
        "        labels = sample_batched['label']\n",
        "        plot_vecs_n_labels(y_tsne,labels,'tsne_hidden_test.png')\n",
        "        x = None\n",
        "        y = None\n",
        "        y_tsne = None\n",
        "        sample_batched = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXHcn2d0HglN"
      },
      "source": [
        "# Boolean variable to control whether to train the linear classifier or not\n",
        "LINEAR = False\n",
        "\n",
        "class LinearNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LinearNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(100, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        return(x)\n",
        "\n",
        "if LINEAR:\n",
        "\n",
        "    if not os.path.exists('linear'):\n",
        "        os.makedirs('linear')\n",
        "\n",
        "    # getting our linear classifier\n",
        "    linear_classifier = LinearNet()\n",
        "\n",
        "    # moving it to device\n",
        "    linear_classifier.to(device)\n",
        "\n",
        "    # using SGD as a linear optimizer\n",
        "    linear_optimizer = optim.SGD(linear_classifier.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-6)\n",
        "\n",
        "    #number of epochs\n",
        "    num_epochs_linear = 10\n",
        "\n",
        "    # Boolean variable to control training of linear classifier\n",
        "    LINEAR_TRAINING = True\n",
        "\n",
        "    # Defining data structures to store train and test info for linear classifier\n",
        "    losses_train_linear = []\n",
        "    acc_train_linear = []\n",
        "    losses_test_linear = []\n",
        "    acc_test_linear = []\n",
        "\n",
        "    # a variable to keep track of the maximum test accuracy, will be useful to store \n",
        "    # model parameters with the best test accuracy\n",
        "    max_test_acc = 0\n",
        "\n",
        "    # if a model exists in the linear folder, load it\n",
        "    if(os.path.isfile(\"linear/model.pth\")):\n",
        "\n",
        "        # load state dict for linear model and optimizer\n",
        "        linear_classifier.load_state_dict(torch.load(\"linear/model.pth\"))\n",
        "        linear_optimizer.load_state_dict(torch.load(\"linear/optimizer.pth\"))\n",
        "\n",
        "        # change learning rate, you can change its values if you don't feel its necessity while training\n",
        "        for g in linear_optimizer.param_groups:\n",
        "          g['lr'] = 0.001\n",
        "          g['weight_decay'] = 0\n",
        "\n",
        "        # load data structures\n",
        "        temp = np.load(\"linear/linear_losses_train_file.npz\")\n",
        "        losses_train_linear = list(temp['arr_0'])\n",
        "        temp = np.load(\"linear/linear_losses_test_file.npz\")\n",
        "        losses_test_linear = list(temp['arr_0'])\n",
        "        temp = np.load(\"linear/linear_acc_train_file.npz\")\n",
        "        acc_train_linear = list(temp['arr_0'])\n",
        "        temp = np.load(\"linear/linear_acc_test_file.npz\")\n",
        "        acc_test_linear = list(temp['arr_0'])\n",
        "\n",
        "    # Run a for loop for training the linear classifier\n",
        "    for epoch in range(num_epochs_linear):\n",
        "\n",
        "        if LINEAR_TRAINING:\n",
        "\n",
        "            # run linear classifier in train mode\n",
        "            linear_classifier.train()\n",
        "\n",
        "            # a list to store losses for each batch in an epoch\n",
        "            epoch_losses_train_linear = []\n",
        "            epoch_acc_train_num_linear = 0.0\n",
        "            epoch_acc_train_den_linear = 0.0\n",
        "\n",
        "            # for loop for running through each batch\n",
        "            for (_, sample_batched) in enumerate(dataloader_training_dataset):\n",
        "\n",
        "                # get x and y from the batch\n",
        "                x = sample_batched['image']\n",
        "                y_actual = sample_batched['label']\n",
        "\n",
        "                # move them to the device\n",
        "                x = x.to(device)\n",
        "                y_actual  = y_actual.to(device)\n",
        "\n",
        "                # get output from resnet architecture\n",
        "                y_intermediate = resnet(x)\n",
        "\n",
        "                # zero the grad values\n",
        "                linear_optimizer.zero_grad()\n",
        "\n",
        "                # run y_intermediate through the linear classifier\n",
        "                y_predicted = linear_classifier(y_intermediate)\n",
        "\n",
        "                # get the cross entropy loss value\n",
        "                loss = nn.CrossEntropyLoss()(y_predicted, y_actual)\n",
        "\n",
        "                # add the obtained loss value to this list\n",
        "                epoch_losses_train_linear.append(loss.data.item())\n",
        "                \n",
        "                # perform backprop through the loss value\n",
        "                loss.backward()\n",
        "\n",
        "                # call the linear_optimizer step function\n",
        "                linear_optimizer.step()\n",
        "\n",
        "                # get predictions and actual values to cpu  \n",
        "                pred = np.argmax(y_predicted.cpu().data, axis=1)\n",
        "                actual = y_actual.cpu().data\n",
        "\n",
        "                #update the numerators and denominators of accuracy\n",
        "                epoch_acc_train_num_linear += (actual == pred).sum().item()\n",
        "                epoch_acc_train_den_linear += len(actual)\n",
        "\n",
        "                x = None\n",
        "                y_intermediate = None\n",
        "                y_predicted = None\n",
        "                sample_batched = None\n",
        "\n",
        "            # update losses and acc lists    \n",
        "            losses_train_linear.append(get_mean_of_list(epoch_losses_train_linear))\n",
        "            acc_train_linear.append(epoch_acc_train_num_linear / epoch_acc_train_den_linear)\n",
        "        \n",
        "        # run linear classifier in eval mode\n",
        "        linear_classifier.eval()\n",
        "\n",
        "        # essential variables to keep track of losses and acc\n",
        "        epoch_losses_test_linear = []\n",
        "        epoch_acc_test_num_linear = 0.0\n",
        "        epoch_acc_test_den_linear = 0.0\n",
        "\n",
        "        # run a for loop through each batch\n",
        "        for (_, sample_batched) in enumerate(dataloader_testing_dataset):\n",
        "            x = sample_batched['image']\n",
        "            y_actual = sample_batched['label']\n",
        "\n",
        "            x = x.to(device)\n",
        "            y_actual  = y_actual.to(device)\n",
        "\n",
        "            y_intermediate = resnet(x)\n",
        "\n",
        "            y_predicted = linear_classifier(y_intermediate)\n",
        "            loss = nn.CrossEntropyLoss()(y_predicted, y_actual)\n",
        "            epoch_losses_test_linear.append(loss.data.item())\n",
        "\n",
        "            pred = np.argmax(y_predicted.cpu().data, axis=1)\n",
        "            actual = y_actual.cpu().data\n",
        "            epoch_acc_test_num_linear += (actual == pred).sum().item()\n",
        "            epoch_acc_test_den_linear += len(actual)\n",
        "\n",
        "        # calculate test_acc\n",
        "        test_acc = epoch_acc_test_num_linear / epoch_acc_test_den_linear\n",
        "        print(test_acc)\n",
        "\n",
        "        if LINEAR_TRAINING:\n",
        "            losses_test_linear.append(get_mean_of_list(epoch_losses_test_linear))\n",
        "            acc_test_linear.append(epoch_acc_test_num_linear / epoch_acc_test_den_linear)\n",
        "\n",
        "            # plotting losses and accuracies\n",
        "\n",
        "            fig = plt.figure(figsize=(10, 10))\n",
        "            sns.set_style('darkgrid')\n",
        "            plt.plot(losses_train_linear)\n",
        "            plt.plot(losses_test_linear)\n",
        "            plt.legend(['Training Losses', 'Testing Losses'])\n",
        "            plt.savefig('linear/losses.png')\n",
        "            plt.close()\n",
        "\n",
        "            fig = plt.figure(figsize=(10, 10))\n",
        "            sns.set_style('darkgrid')\n",
        "            plt.plot(acc_train_linear)\n",
        "            plt.plot(acc_test_linear)\n",
        "            plt.legend(['Training Accuracy', 'Testing Accuracy'])\n",
        "            plt.savefig('linear/accuracy.png')\n",
        "            plt.close()\n",
        "\n",
        "            print(\"Epoch completed\")\n",
        "\n",
        "            if test_acc >= max_test_acc:\n",
        "\n",
        "                # save the model only when test_acc exceeds the current max_test_acc\n",
        "\n",
        "                max_test_acc = test_acc\n",
        "                torch.save(linear_classifier.state_dict(), 'linear/model.pth')\n",
        "                torch.save(linear_optimizer.state_dict(), 'linear/optimizer.pth')\n",
        "\n",
        "        # save data structures\n",
        "        np.savez(\"linear/linear_losses_train_file\", np.array(losses_train_linear))\n",
        "        np.savez(\"linear/linear_losses_test_file\", np.array(losses_test_linear))\n",
        "        np.savez(\"linear/linear_acc_train_file\", np.array(acc_train_linear))\n",
        "        np.savez(\"linear/linear_acc_test_file\", np.array(acc_test_linear))\n",
        "\n",
        "# Afunction to get PIL image from tensor\n",
        "\n",
        "def deprocess_and_show(img_tensor):\n",
        "     return T.Compose([\n",
        "             T.Normalize((0, 0, 0), (2, 2, 2)),\n",
        "             T.Normalize((-0.5, -0.5, -0.5), (1, 1, 1)),\n",
        "             T.ToPILImage()\n",
        "           ])(img_tensor)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SccIpW7RD0Fd"
      },
      "source": [
        "**********************"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B459-KluD8Zd"
      },
      "source": [
        "def get_color_distortion(s=1.0):\n",
        "    # s is the strength of color distortion.\n",
        "    color_jitter = T.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
        "    rnd_color_jitter = T.RandomApply([color_jitter], p=0.8)\n",
        "    rnd_gray = T.RandomGrayscale(p=0.2)\n",
        "    color_distort = T.Compose([rnd_color_jitter, rnd_gray])\n",
        "    return color_distort\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, root_dir, filenames, labels, mutation=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.file_names = filenames\n",
        "        self.labels = labels\n",
        "        self.mutation = mutation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def tensorify(self, img):\n",
        "        res = T.ToTensor()(img)\n",
        "        res = T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(res)\n",
        "        return res\n",
        "\n",
        "    def mutate_image(self, img):\n",
        "        res = T.RandomResizedCrop(224)(img)\n",
        "        res = get_color_distortion(1)(res)\n",
        "        return res\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir, self.file_names[idx])\n",
        "        image = Image.open(img_name)\n",
        "        label = self.labels[idx]\n",
        "        image = T.Resize((250, 250))(image)\n",
        "\n",
        "        if self.mutation:\n",
        "            image1 = self.mutate_image(image)\n",
        "            image1 = self.tensorify(image1)\n",
        "            image2 = self.mutate_image(image)\n",
        "            image2 = self.tensorify(image2)\n",
        "            sample = {'image1': image1, 'image2': image2, 'label': label}\n",
        "        else:\n",
        "            image = T.Resize((224, 224))(image)\n",
        "            image = self.tensorify(image)\n",
        "            sample = {'image': image, 'label': label}\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvQiE_UUEA5h"
      },
      "source": [
        "def loss(a,b):\n",
        "    a_norm = torch.norm(a,dim=1).reshape(-1,1)\n",
        "    a_cap = torch.div(a,a_norm)\n",
        "    b_norm = torch.norm(b,dim=1).reshape(-1,1)\n",
        "    b_cap = torch.div(b,b_norm)\n",
        "    a_cap_b_cap = torch.cat([a_cap,b_cap],dim=0)\n",
        "    a_cap_b_cap_transpose = torch.t(a_cap_b_cap)\n",
        "    b_cap_a_cap = torch.cat([b_cap,a_cap],dim=0)\n",
        "    sim = torch.mm(a_cap_b_cap,a_cap_b_cap_transpose)\n",
        "    sim_by_tau = torch.div(sim,tau)\n",
        "    exp_sim_by_tau = torch.exp(sim_by_tau)\n",
        "    sum_of_rows = torch.sum(exp_sim_by_tau, dim=1)\n",
        "    exp_sim_by_tau_diag = torch.diag(exp_sim_by_tau)\n",
        "    numerators = torch.exp(torch.div(torch.nn.CosineSimilarity()(a_cap_b_cap,b_cap_a_cap),tau))\n",
        "    denominators = sum_of_rows - exp_sim_by_tau_diag\n",
        "    num_by_den = torch.div(numerators,denominators)\n",
        "    neglog_num_by_den = -torch.log(num_by_den)\n",
        "    return torch.mean(neglog_num_by_den)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEllhjdVEEOj"
      },
      "source": [
        "resnet = resnet18(pretrained=False)\n",
        "\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "    ('fc1', nn.Linear(resnet.fc.in_features, 100)),\n",
        "    ('added_relu1', nn.ReLU(inplace=True)),\n",
        "    ('fc2', nn.Linear(100, 50)),\n",
        "    ('added_relu2', nn.ReLU(inplace=True)),\n",
        "    ('fc3', nn.Linear(50, 25))\n",
        "]))\n",
        "\n",
        "resnet.fc = classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM7J4g1-M25r"
      },
      "source": [
        "## Visualization of the process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "T5gx2dEOEIIA",
        "outputId": "241470ba-b090-4414-fa92-109de44f5016"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "tsne = TSNE()\n",
        "\n",
        "def plot_vecs_n_labels(v,labels,fname):\n",
        "    fig = plt.figure(figsize = (10, 10))\n",
        "    plt.axis('off')\n",
        "    sns.set_style(\"darkgrid\")\n",
        "    sns.scatterplot(v[:,0], v[:,1], hue=labels, legend='full', palette=sns.color_palette(\"bright\", 5))\n",
        "    plt.legend(['car', 'dog', 'elephant','cat','airplane'])\n",
        "    plt.savefig(fname)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    \n",
        "for (_, sample_batched) in enumerate(dataloader_training_dataset):\n",
        "    x = sample_batched['image']\n",
        "    x = x.to(device)\n",
        "    y = resnet(x)\n",
        "    y_tsne = tsne.fit_transform(y.cpu().data)\n",
        "    labels = sample_batched['label']\n",
        "    plot_vecs_n_labels(y_tsne,labels,'tsne_train_last_layer.png')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}